# Part1
#### 1. 모델의 교차검증의 의미와 방법에 대하여 설명하시오.
- 모델의 일반화 오차를 줄이기 위해 테스트 세트로 여러번 평가하면 모델이 테스트 세트에 최적화 된다
- 하지만 테스트 세트에 최적화 되는 것이 항상 일반화 성능이 뛰어난 것이라 할 수 없다
- 그러므로 검증 세트를 통해 훈련 세트 내에서 다양한 하이퍼파라미터로 여러 모델을 훈련시키고 검증 세트에서 최상의 성능을 내는 모델과 하이퍼파라미터를 선택할 수 있다
- 그리고 검증 세트를 통해 선택된 모델에 단 한번의 최종 평가로써 테스트 세트를 사용한다
- 이 때, 훈련 데이터 세트를 일정한 크기로 여러 덩어리로 분할한 뒤 훈련/검증 데이터를 바꿔 가면서 여러번 테스트할 수 있는 교차검증 기법을 사용한다
- 그리고 여러번 평가된 성능을 평균을 내서 모델을 평가한다
- 이렇게 교차검증을 사용하면 한 번만 나누어서 학습/검증 절차를 거치는 것에 비해 더 일반화된 성능을 얻을 수 있다
---  
#### 2. 모델의 과대적합, 과소적합, 일반화의 정의를 서술하고, 1차원 특성을 갖는 수학적 예제를 들어 과대적합, 과소적합, 일반화를 설명하시오.
- 과대적합(Over Fitting)
  - 과대적합이란 훈련 데이터에 너무 최적화 되어 모델이 매우 복잡하고 일반화 성능이 매우 떨어지는 상태를 말한다
  - 과대적합된 모델은 일반적으로 훈련 데이터에 대한 오차는 적으나 새로운 데이터에 대한 오차가 크다
  - 예를들어 $y=sin(x)$를 따르는 실제 데이터 분포에서 몇 개의 훈련 데이터만 가지고 있다고 했을 때, 과대적합 모델은 이 몇 개의 훈련 데이터를 가지고 $y=x^3$형태의 모델을 만들 수 있다
  - 이렇게 되면 훈련 데이터에 대한 오차는 거의 0에 가깝지만 다른 $sin(x)$데이터가 들어왔을 때 오차가 매우 크다
- 과소적합(Under Fitting)
  - 과소적합이란 훈련 데이터가 덜 학습되어 모델이 단순하고 일반화 성능이 매우 떨어지는 상태를 말한다
  -  과소적합된 모델은 일반적으로 훈련 데이터 및 새로운 데이터 모두에 대한 오차가 크다
  -  위의 예와 마찬가지로 $y=sin(x)$를 따르는 실제 데이터 분포에서 몇 개의 훈련 데이터만 가지고 있다고 했을 때, 과소적합 모델은 $y=x$형태의 모델을 만들 수 있다
  -  이러면 훈련 데이터에 대한 오차율도 낮지 않을 뿐더러, 새로운 $sin(x)$데이터가 들어왔을 때 오차도 크다
-  일반화(Generalization)
   -  모델이 훈련을 완료하고 새로운 데이터가 들어왔을때 새로운 데이터에 대한 예측을 실제 값과 맞게 수행하는 것이 일반화 성능이다
   -  위의 예와 마찬가지로 $y=sin(x)$를 따르는 실제 데이터  분포에서 몇 개의 훈련 데이터만 가지고 있다고 했을 때, 적절한 규제와 하이퍼파라미터 조절을 통해 $sin(x)$그래프와 비슷한 모델을 만들어 낸다면 일반화 성능이 좋다고할 수 있다
---
#### 3. 분류에서 오차 행렬에 대하여 설명하고,이를 통하여 분류기의 성능 측정할 수 있는 정밀도와 재현율에 대하여 설명하시오.
| |Actually Positive|Actually Negative|
|---|---|---|
|**Predicted Positive**|True Positive(TP)|False Positive(FP)|
|**Predicted Negative**|False Negative(FN)|True Negative(TN)|
- 분류 문제에서 모델이 예측하는 예측값은 실제 데이터가 가질 수 있는 값 2개(Positive, Negative)와 모델이 예측할 수 있는 값 2개(Positive, Negative)를 통해 위 표에서 볼 수 있듯이 총 4개($2^2$)의 경우가 있다
- 정밀도(Precision)
  - 정밀도는 다음과 같은 수식을 통해 계산한다
  - $\frac{TP}{TP+FP}$
  - 정밀도는 모델이 맞다고 예측한 것 중에 실제 정답의 비율이다
  - 다시말해, 정밀도는 모델 입장의 정확도(accuracy)이다
- 재현율(Recall)
  - 재현율은 다음과 같은 수식을 통해 계산한다
  - $\frac{TP}{TP+FN}$
  - 재현율은 실제 데이터가 정답인 것 중에 모델이 정답을 맞춘 비율이다
  - 다시말해, 재현율은 데이터 입장의 정확도(accuracy)이다
- 정밀도와 재현율은 서로 Trade-off 관계에 있다
---
#### 4. 훈련 세트에 있는 특성들이 아주 다른 스케일을 가지고 있다고 하자.이런 데이터에 잘 작동하지 않는 회귀 알고리즘은 무엇인가? 그 이유를 설명하고 이 문제를 해결할 방법을 제시하시오.
- 릿지(Ridge), 라쏘(Lasso), 엘라스틱넷(Elastic Net) 등의 규제가 있는 회귀 알고리즘들
- 파라미터 계수($\theta$)에 규제를 가할때 입력 데이터($x$) 자체가 크다면 자연히 파라미터 계수($\theta$)가 작아지게 된다 
- 따라서 입력 데이터의 스케일에 따라 원치않는 규제가 추가적으로 가해질 수 있으므로 규제가 있는 모델들은 반드시 스케일을 조절해 주어야 한다
---
#### 5. 다음과 같이 사용해야 하는 이유를 설명하시오. 
1) 규제가 없는 선형 모델 대신 릿지 회귀
   - 규제가 없는 선형 모델은 훈련 데이터에 과대적합될 수 있으므로 규제가 없는 선형 모델보다 릿지 회귀를 통해 좀 더 일반화 된 모델을 만드는 것이 좋다
2) 릿지 회귀 대신 라쏘 회기
   - 릿지 회귀는 모델의 가중치가 완전히 사라지지 않고 점진적으로 줄어드는 반면, 라쏘 회귀는 모델의 가중치중 필요 없는 가중치가 완전히 사라지는 효과가 있다
   - 따라서 라쏘 회귀는 데이터 특성중 일부분만 중요한 정보를 가지는 데이터에 사용하면 효과적이다
3) 라쏘 회귀 대신 엘라스틱넷
   - 모델을 학습하는 대부분의 경우에서 규제가 있는 모델이 없는 모델보다 더 좋은 결과를 얻는다
   - 기본적으로는 릿지 회귀를 많이 사용하지만 특성수가 적거나 특성 중 일부 특성만이 중요한 정보를 가지는 데이터에 대해서는 라쏘 회귀가 더 적합하다
   - 하지만  라쏘 회귀는 모델의 가중치를 0으로 만들어서 특성을 제거해 버리는 특징이 있으므로 특성 수가 훈련 샘플 수보다 많거나 여러 특성이 서로 연관되어 있는 경우 엘라스틱 넷을 사용함
   - 왜냐하면 엘라스틱 넷은 릿지와 라쏘의 볼록조합으로 구성되어 두 규제를 적절히 사용할 수 있음
---
#### 6. 사진을 낮과 밤,실내와 실외로 분류하려 한다.다음 중 어떤 분류기를 사용해야 하는 지 선택하고 그 이유를 설명하시오.
- 두 개의 로지스틱 회귀 분류기
   - 사진을 분류하기 위해서 사진의 픽셀 하나하나를 특성으로 보고 학습을 시작한다 
   - 즉, 사진을 N x M 행렬의 픽셀로 표현한다면 N x M 차원의 특성을 가진 벡터를 학습하는 것이다
   - 이 때, 사진을 낮과 밤, 실내와 실외로 분류하려 한다면 낮과 밤을 가르는 특성과 실내와 실외로 분류하는 특성은 서로 다른 특성들이다
   - 따라서 두 개의 로지스틱 회귀 분류기를 통해 한 분류기는 사진의 낮과 밤을 구분하고 다른 한 분류기는 실내와 실외로 구분하도록 학습한다면 소프트맥스 회귀 분류기 보다 더 잘 분류할 수 있다
   - 소프트맥스 회귀 분류기는 같은 특성을 가지고 여러개의 클래스를 분류하는 문제에 더 적합하다
   - 예를들면 사진을 낮과 밤 뿐만 아니라 새벽, 오전, 오후와 같이 같은 특성들의 특징을 통해 분류를 하는 문제들이 이에 해당된다
---  
#### 7. 서포트 벡터머신의 기본 아이디어를 서술하시오.
- 서포트 벡터 머신(Support Vector Machine)의 기본 아이디어를 한마디로 요약하자면 "서포트 벡터(Support Vector)에서 가장 멀리 떨어진 결정 경계(Decision Boundary)를 찾는다" 이다
- 즉, 결정 경계의 제일 가까운 훈련 샘플로부터 가능한한 멀리 떨어져 있는 상태인 라지 마진(large margin) 상태의 결정 경계를 찾는 것이다
- 서포트 벡터란 서로 다른 두 클래스를 가르는 결정 경계에서 가장 가까이 위치한 두 벡터를 말한다
- 서포트 벡터보다 더 결정 결계에서 먼 벡터들은 결정 경계를 정하는데 영향을 미치지 않는다
- 서포트 벡터 머신은 결정 경계와 서포트 벡터 사이에 벡터를 허용하지 않는 하드 마진 분류와 어느정도 오류를 허용하는 소프트 마진 분류로 나뉜다
- 하드 마진 분류는 이상치에 매우 민감하므로 대부분 소프트 마진 분류를 사용한다 
- 소프트 마진 분류는 하이퍼파라미터 $C$를 통해 허용하는 오류를 조절함
- 선형 SVM과 달리 비선형 SVM은 다항식 특성을 많이 추가해야함
- 다항식 특성을 추가하는 것은 훈련을 매우 느리게 만듦
- SVM에서는 이를 극복하기 위해 커널 트릭을 사용
---
#### 8. 가우시안 RBF 커널을 사용해 SVM 분류기를 훈련시켰더니 훈련 세트에 과대적합이 되었다. 과대적합을 완화하기 위해 이 모델의 하이퍼파라미터 $\gamma$와 C를 튜닝하는 방법을 제시하시오.
- 가우시안 커널을 이용한 SVM 분류기는 하이퍼파라미터 $\gamma$와 C를 조절하여 적절한 결정 경계를 선택할 수 있다
- 우선 C는 가우시안 커널이 아닌 기본 SVM에서 조절 가능한 하이퍼파라미터로써 C가 크면 클수록 마진 오류를 허용하지 않는다
- 따라서 SVM이 과대적합 되어 있다면 적절히 C를 줄여서 마진 오류를 조금 더 허용하는 쪽으로 조절해 줘야 한다
- $\gamma$는 가우시안 커널을 사용할 때 사용하는 하이퍼파라미터로써 $\gamma$가 크면 클수록 데이터를 사영하는 가우시안 분포의 폭이 좁아진다
- 다시말해 $\gamma$는 가우시안 커널의 분산의 역수이다
- 따라서 SVM이 과대적합 되어 있다면 적절히 $\gamma$를 줄여서 데이터를 사영하는 가우시안 분포의 분산을 늘려야 한다